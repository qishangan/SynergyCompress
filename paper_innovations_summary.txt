# 研究总结与创新点分析报告

## 1. 研究背景与文献脉络

- 知识蒸馏已成为压缩大型预训练语言模型的常用手段，DistilBERT 等工作验证了在保持泛化性能的同时显著降低模型规模的可行性 [1]。
- 剪枝技术正在向“渐进式稀疏化 + 细致微调”演进，以缓解直接大幅稀疏化造成的性能崩塌；Movement Pruning 和 SparseGPT 等研究强调了调度与恢复阶段的重要性 [2][3]。
- 低比特（尤其是 4-bit）量化在近期工作中逐渐成熟，SmoothQuant 与 QLoRA 展示了对大模型进行低比特量化并保持精度的路径，为实际部署奠定了基础 [4][5]。
- 对压缩效果的评价正回到“计算量与吞吐”这一核心指标，The State of Sparsity 等研究呼吁在报告稀疏模型时同步给出 FLOPs 下降情况 [6]。

## 2. 本实验流程与关键观察

本仓库复现并串联了“蒸馏 → 渐进剪枝 → 低学习率恢复微调 → 4-bit 量化”的四阶段流程，围绕 SST-2 分类任务开展实验。核心观察如下：

- 蒸馏后的 DistilBERT 学生模型在基线验证集上达到 0.9002 的准确率及 0.6813 GFLOPs 的理论计算成本，为后续压缩提供了高质量起点。
- 采用多项式调度的渐进式结构化剪枝结合低学习率恢复微调后，模型在保持 0.8888 准确率的同时将 GFLOPs 降至 0.5450，约等于 20% 的理论计算量降低。
- 量化仅带来轻微的模型大小波动（剪枝 + 量化模型约 68.7 MB），表明在已有稀疏结构上进行 4-bit 权重量化不会显著增加额外存储开销。

## 3. 与文献对比的增量贡献

- **流程验证的客观证据**：实验数据表明，在 DistilBERT 这类中等规模模型上，按“蒸馏优先、剪枝居中、量化收尾”的顺序可以达成与 SmoothQuant、QLoRA 等工作中相近的低比特量化效果，同时获得约 20% 的 GFLOPs 下降，验证了将渐进剪枝嵌入标准蒸馏-量化链路的实用价值。
- **调参经验的显式记录**：通过实验定位到对稀疏度目标的多项式增速与恢复阶段的 5e-5 量级学习率组合，可以在 20% 稀疏度下保持 <2% 的准确率损失，为复现 Movement Pruning 等研究理念提供了小模型情境下的可操作范式。
- **以 GFLOPs 为核心的评估实践**：结果报告与可视化围绕 “Accuracy vs. GFLOPs” 展开，呼应了近期文献对真实算力收益的关注，并为社区提供了可直接对比的指标格式。

## 4. 局限与后续方向

- 当前实验基于单一下游任务和单一学生模型，仍需在多任务、多尺度模型上检验结论的稳健性。
- 剪枝阶段主要采用结构化稀疏约束，尚未探索与硬件友好的 2:4 稀疏、LoRA 适配等组合策略。
- 量化部分聚焦权重量化，后续可引入 SmoothQuant 所强调的激活平滑与联合量化策略，加深与主流推理加速方案的对接。

## 参考文献

[1] Sanh, V., Debut, L., Chaumond, J., & Wolf, T. (2019). DistilBERT: a distilled version of BERT: smaller, faster, cheaper and lighter. arXiv:1910.01108.  
[2] Sanh, V., Wolf, T., & Rush, A. M. (2020). Movement Pruning: Adaptive Sparsity by Fine-Tuning. arXiv:2005.07683.  
[3] Frantar, E., & Alistarh, D. (2023). SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot. arXiv:2301.00774.  
[4] Xiao, S., Zhang, S., Chen, S., et al. (2023). SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models. arXiv:2211.10438.  
[5] Dettmers, T., Lewis, M., Shleifer, S., & Zettlemoyer, L. (2023). QLoRA: Efficient Finetuning of Quantized LLMs. arXiv:2305.14314.  
[6] Gale, T., Elsen, E., & Hooker, S. (2019). The State of Sparsity in Deep Neural Networks. arXiv:1902.09574.
